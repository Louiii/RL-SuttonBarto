from PDFmaker import *
from SampleBandits import *
import numpy as np

selectionMethods = ["Random", "εGreedy", "Softmax", "UCB", "Gradient Bandit", "Decaying εGreedy"]

class QSample():
    def __init__(self):
        self.q=0
        self.visits=0

class ActionSelection():
    """All of the functions return an action (a choice of the bandit)"""
    def __init__(self, actions):
        self.actions=actions
        self.Q={a:QSample() for a in actions}

        self.AverageR = 0
        self.H = np.zeros(len(actions))

    def randomChoice(self):
        return random.choice(self.actions)

    def εGreedy(self, ε=0.1):
        if random.random() < ε:
            return self.randomChoice()
        else:
            return self.maxAction()

    def maxAction(self):# returns the action with the highest Q-value
        Qs=[(action, self.Q[action].q) for action in self.actions]
        mx = max(Qs, key=lambda x:x[1])[1]
        mxlt = [a for (a,q) in Qs if q==mx]
        return mxlt[np.random.randint(len(mxlt))]

    def softmax(self, τ):
        """ τ is a positive param called temp.
        high τ ~ equiprobable actions
        low τ ~ best actions most probable
        using a pdf defined by the function: k ^ Q-value-of-action, (after normalisation).
        requires knowledge of Q-values/powers of e
        """
        Qs = np.array([self.Q[a].q/τ for a in self.actions])
        return np.random.choice(np.arange(len(actions)), p=np.exp(Qs)/np.sum(np.exp(Qs)))#self.samplePolicy(π)

    def UCB(self, iteration, c=2):#greedy upper confidence bound
        # pass in an entry of the qtable
        """ c must be picked to fit the magnitude of rewards on the problem
        c > 0 controls the degree of exploration
        """
        t = iteration+1 # sum([self.Q[(state, a)].visits for a in self.actions])
        ucb = lambda Q_: Q_.q + c * (np.log(t)/Q_.visits)**0.5
        bounds = []
        for a in self.actions:
            if self.Q[a].visits == 0:
                return a
            bounds.append( (a, ucb( self.Q[a] ) ) )
        actionBound = max(bounds, key=lambda x:x[1])
        return actionBound[0]

    def decayεGreedy(self, iteration):
        ε = 0.01 + 1 / (iteration+1)**0.2
        if random.random() < ε:
            return self.randomChoice()
        else:
            return self.maxAction()

    def gradientBandit(self):
        expnt = np.exp(self.H)
        self.π = expnt / np.sum(expnt)
        return np.random.choice(np.arange(len(self.actions)), p=self.π)

    def gradientBanditUpdate(self, reward, action, α, t, gradient_baseline):
        self.AverageR = t / (t + 1) * self.AverageR + reward / (t + 1)

        one_hot = np.zeros(len(self.actions))
        one_hot[action] = 1
        baseline = self.AverageR if gradient_baseline else 0
        self.H += α * (reward - baseline) * (one_hot - self.π)


