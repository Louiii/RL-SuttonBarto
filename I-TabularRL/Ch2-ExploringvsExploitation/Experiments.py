from SelectionMethods import *
from tqdm import tqdm

def figure_2_2():

    εs = [0, 0.01, 0.1]
    steps = 1000
    repeats = 2000
    totalReward=0
    opt_action = len(actions)

    rewards = np.zeros((len(εs), steps))
    opts    = np.zeros((len(εs), steps))
    for _ in tqdm(range(repeats)):
        for εi, ε in enumerate(εs):
            agent = ActionSelection(actions)
            for iteration in range(steps):
                action = agent.εGreedy(ε=ε)
                reward = performAction(action)

                agent.Q[action].visits += 1
                agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward
                
                rewards[εi, iteration] += reward/repeats
                if action==opt_action: opts[εi, iteration] += 100/repeats
    labels = ['ε = '+str(ε) for ε in εs]
    data_a = {l:(list(range(1, steps+1)), rewards[li]) for li, l in enumerate(labels)}
    data_b = {l:(list(range(1, steps+1)), opts[li]) for li, l in enumerate(labels)}
    
    # print(data)
    multipleCurvesPlot(data_a, "", "Steps", "Average reward", "figure_2_2a", lgplc='lower right',
                        w=10.0, h=4.5, ylims=None, labels=labels)
    multipleCurvesPlot(data_b, "", "Steps", "% Optimal action", "figure_2_2b", lgplc='lower right',
                        w=10.0, h=4.5, ylims=None, labels=labels)


def figure_2_3(my_bandits=False):
    labels = ['Optimistic, greedy\n$Q_{1}$ = 5, ε = 0', 'Realistic, ε-greedy\n$Q_{1}$ = 0, ε = 0.1']
    εs = [0, 0.1]
    steps = 1000
    repeats = 2000
    means = [0.3, -0.7, 1.5, 0.5, 1.2, -1.4, -0.2, -1.2, 0.8, -0.5]
    best_action = 3
    α = 0.1
    opt_action = len(actions)
    opts = np.zeros((len(εs), steps))
    for _ in tqdm(range(repeats)):
        for εi, ε in enumerate(εs):
            agent = ActionSelection(actions)
            if ε==0: 
                for a in actions: agent.Q[a].q = 5
            if my_bandits:
                for iteration in range(steps):
                    action = agent.εGreedy(ε=ε)
                    reward = performAction(action)

                    agent.Q[action].visits += 1
                    agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward

                    if action==opt_action: opts[εi, iteration] += 100/repeats
            else:
                for iteration in range(steps):
                    action = agent.εGreedy(ε=ε)
                    reward = np.random.normal(means[action-1])

                    agent.Q[action].q += α*(reward - agent.Q[action].q)
                    if action==best_action: opts[εi, iteration] += 100/repeats
    data = {l:(list(range(1, steps+1)), opts[li]) for li, l in enumerate(labels)}
    
    fname = "figure_2_3_my_bandits" if my_bandits else "figure_2_3"
    multipleCurvesPlot(data, "", "Steps", "%\nOptimal action", fname, lgplc='lower right',
                        w=10.0, h=4.5, ylims=None, labels=labels)
    

def figure_2_4(my_bandits=False):
    labels = ['UCB, c = 2', 'ε-greedy, ε = 0.1']
    steps = 1000
    repeats = 2000
    α = 0.1
    means = [0.3, -0.7, 1.5, 0.5, 1.2, -1.4, -0.2, -1.2, 0.8, -0.5]
    opts = np.zeros((2, steps))
    for _ in tqdm(range(repeats)):
        if my_bandits:
            agent = ActionSelection(actions)
            for iteration in range(steps):
                action = agent.UCB(iteration)
                reward = performAction(action)

                agent.Q[action].visits += 1
                agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward

                opts[0, iteration] += reward/repeats
            agent = ActionSelection(actions)
            for iteration in range(steps):
                action = agent.εGreedy(ε=0.1)
                reward = performAction(action)

                agent.Q[action].visits += 1
                agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward

                opts[1, iteration] += reward/repeats
        else:
            agent = ActionSelection(actions)
            for iteration in range(steps):
                action = agent.UCB(iteration)
                reward = np.random.normal(means[action-1])

                agent.Q[action].visits += 1
                agent.Q[action].q += α*(reward - agent.Q[action].q)
                opts[0, iteration] += reward/repeats
            agent = ActionSelection(actions)
            for iteration in range(steps):
                action = agent.εGreedy(ε=0.1)
                reward = np.random.normal(means[action-1])

                agent.Q[action].visits += 1
                agent.Q[action].q += α*(reward - agent.Q[action].q)
                opts[1, iteration] += reward/repeats
    data = {l:(list(range(1, steps+1)), opts[li]) for li, l in enumerate(labels)}
    
    fname = "figure_2_4_my_bandits" if my_bandits else "figure_2_4"
    multipleCurvesPlot(data, "", "Steps", "Average reward", fname, lgplc='lower right',
                        w=10.0, h=4.5, ylims=[-0.05, 1.5], labels=labels)

def figure_2_5(my_bandits=False):
    labels = ['With baseline, α = 0.1', 'With baseline, α = 0.4', 'Without baseline, α = 0.1', 'Without baseline, α = 0.4']
    steps = 1000
    repeats = 2000
    means = [0.3, -0.7, 1.5, 0.5, 1.2, -1.4, -0.2, -1.2, 0.8, -0.5]
    best_action = 3
    αs = [0.1, 0.4]
    opt_action = len(actions)
    opts = np.zeros((4, steps))
    for _ in tqdm(range(repeats)):
        for αi, α in enumerate(αs):
            if my_bandits:
                agent = ActionSelection(actions)
                for iteration in range(steps):
                    action = agent.gradientBandit()
                    reward = performAction(action) + 4

                    agent.gradientBanditUpdate(reward, action, α, iteration, True)

                    if action==opt_action: opts[αi, iteration] += 100/repeats
                agent = ActionSelection(actions)
                for iteration in range(steps):
                    action = agent.gradientBandit()
                    reward = performAction(action) + 4

                    agent.gradientBanditUpdate(reward, action, α, iteration, False)

                    if action==opt_action: opts[αi+2, iteration] += 100/repeats
            else:
                agent = ActionSelection(actions)
                for iteration in range(steps):
                    action = agent.gradientBandit()
                    reward = np.random.normal(means[action-1]) + 4

                    agent.gradientBanditUpdate(reward, action, α, iteration, True)

                    if action==best_action: opts[αi, iteration] += 100/repeats
                agent = ActionSelection(actions)
                for iteration in range(steps):
                    action = agent.gradientBandit()
                    reward = np.random.normal(means[action-1]) + 4

                    agent.gradientBanditUpdate(reward, action, α, iteration, False)

                    if action==best_action: opts[αi+2, iteration] += 100/repeats
    data = {l:(list(range(1, steps+1)), opts[li]) for li, l in enumerate(labels)}
    
    fname = "figure_2_5_my_bandits" if my_bandits else "figure_2_5"
    multipleCurvesPlot(data, "", "Steps", "%\nOptimal action", fname, lgplc='lower right',
                        w=10.0, h=4.5, ylims=None, labels=labels)


def figure_2_6(my_bandits=False):
    labels = ['ε-greedy', 'gradient bandit', 'UCB', 'greedy with optimistic initialisation α = 0.1']
    steps = 1000
    repeats = 500
    means = [0.3, -0.7, 1.5, 0.5, 1.2, -1.4, -0.2, -1.2, 0.8, -0.5]

    xs = 1/np.power(2, np.arange(8))
    xs = np.array( list(reversed(list(xs))) + [2, 4] )#[1/128, 1/64, .., 2, 4]

    εs = np.copy(xs[:6])
    αs = np.copy(xs[2:])
    cs = np.copy(xs[3:])
    Q0s = np.copy(xs[5:])
    εrewards = np.zeros(len(εs))
    αrewards = np.zeros(len(αs))
    crewards = np.zeros(len(cs))
    qrewards = np.zeros(len(Q0s))
    for _ in tqdm(range(repeats)):
        for li, l in enumerate(labels):
            if l=='ε-greedy':
                for εi, ε in enumerate(εs):
                    agent = ActionSelection(actions)
                    totalreward = 0
                    for iteration in range(steps):
                        action = agent.εGreedy(ε=ε)
                        reward = np.random.normal(means[action-1]) if not my_bandits else performAction(action)

                        agent.Q[action].visits += 1
                        agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward
                        totalreward += reward
                    εrewards[εi] += totalreward/(repeats*steps)
            elif l=='gradient bandit':
                for αi, α in enumerate(αs):
                    agent = ActionSelection(actions)
                    totalreward = 0
                    for iteration in range(steps):
                        action = agent.gradientBandit()
                        reward = np.random.normal(means[action-1]) + 4 if not my_bandits else performAction(action) + 4

                        agent.gradientBanditUpdate(reward, action, α, iteration, True)
                        totalreward += reward
                    αrewards[αi] += totalreward/(repeats*steps) - 4/repeats
            elif l=='UCB':
                for ci, c in enumerate(cs):
                    agent = ActionSelection(actions)
                    totalreward = 0
                    for iteration in range(steps):
                        action = agent.UCB(iteration, c=c)
                        reward = np.random.normal(means[action-1]) if not my_bandits else performAction(action)

                        agent.Q[action].visits += 1
                        agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward

                        totalreward += reward
                    crewards[ci] += totalreward/(repeats*steps)

            elif l=='greedy with optimistic initialisation α = 0.1':
                for Q0i, Q0 in enumerate(Q0s):
                    agent = ActionSelection(actions)
                    totalreward = 0
                    for a in actions: 
                        agent.Q[a].q = Q0
                    for iteration in range(steps):
                        action = agent.maxAction()
                        reward = np.random.normal(means[action-1]) if not my_bandits else performAction(action)

                        # agent.Q[action].visits += 1
                        # agent.Q[action].q += (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward
                        agent.Q[action].q = 0.9*agent.Q[action].q + 0.1*reward

                        totalreward += reward
                    qrewards[Q0i] += totalreward/(repeats*steps)

    xaxs = [εs, αs, cs, Q0s]
    rewards = [εrewards, αrewards, crewards, qrewards]
    data = {l:(list(xaxs[li]), list(rewards[li])) for li, l in enumerate(labels)}
    
    ylm = [1, 1.5] if not my_bandits else [0.35, 0.6]
    fname = "figure_2_6_my_bandits" if my_bandits else "figure_2_6"
    multipleCurvesPlot(data, "", "ε, α, c, $Q_0$", "Average reward over\nthe first 1000 steps", fname, lgplc='lower right',
                        w=10.0, h=4.5, ylims=ylm, labels=labels, xlog=(xs, ('1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1', '2', '4')) )


def performAction(action):
    return sampleBandit(bandits[action-1])

def simulate(iterations, rec, chosen):
    Qvalues=[]
    totalReward=0
    agent = ActionSelection(actions)
    # for a in actions: Q[a] = QSample()
    τ=100
    α=0.1
    # for episode in tqdm(range(episodes)):
    for iter in range(iterations):
        if chosen=="Random":
            action = agent.randomChoice()
        elif chosen=="εGreedy":
            action = agent.εGreedy(ε=0.2)
        elif chosen=="Softmax":
            action = agent.softmax(τ, e=np.exp(1))#actionSelection()
            τ*=0.9995
            # print(τ)
        elif chosen=="UCB":
            action = agent.UCB(iter)
        elif chosen=="Decaying εGreedy":
            action = agent.decayεGreedy(iter)
        elif chosen=="Gradient Bandit":
            action = agent.gradientBandit()

        reward = performAction(action)

        if chosen=="Gradient Bandit": agent.gradientBanditUpdate(reward, action, α, iter)

        # if chosen=="Gradient Bandit":
        #     if gradientBaseline:
        #         baseline = agent.AverageR
        #     else:
        #         baseline = 0
        #     agent.Q[action].q += α * (reward - baseline) * (1 - agent.policy(agent.H)[action])
        # elif constUpdate:
        #     """ update estimation with constant step size"""
        #     agent.Q[action].q += α * (reward - agent.Q[action].q)
        # else:
        """Update rule: Sample average"""
        agent.Q[action].visits += 1
        agent.Q[action].q = (1 - 1/(1 + agent.Q[action].visits))*agent.Q[action].q + (1/(1 + agent.Q[action].visits))*reward

        if iter%rec==0:
            Qvalues.append([agent.Q[action].q for action in actions])
        totalReward += reward
    return np.array(Qvalues), totalReward

def samplePlot():
    designNewBandits(0.1)
    data = makeData(100000)

    violin(data)
    plotHists(data)



def plotPolicies():
    for chosen in selectionMethods:
        Qvalues, totalReward = simulate(it, rec, chosen)

        print(Qvalues[-1])
        print("True values: "+str(trueValues))

        sea(Qvalues, rec, chosen)

def optimalitySimulation(it, rec, name):
    allErrors = {}
    for chosen in selectionMethods:
        opt = {}
        for i in range(0, it, rec): opt[i]=0
        repeats=100
        for _ in range(repeats):
            Qvalues, _ = simulate(it, rec, chosen)
            # op = [1 for x in Qvalues if max(x) == x[nBandits-1] else 0]
            for i in range(len(Qvalues)):
                if max(Qvalues[i]) == Qvalues[i][nBandits-1]:
                    opt[i] += 1/repeats

        # errors = []
        # for vals in Qvalues:
        #     errors.append( sum([ np.abs(trueValues[i]-vals[i]) for i in range(nBandits)])/nBandits )
        allErrors[chosen] = [opt[i] for i in range(0, it, rec)]
    export_dataset(allErrors, name)

def my_plots():
    # samplePlot()
    constUpdate = False
    gradientBaseline = True


    it, rec = 1000, 10
    

    # plotPolicies()

    name='itr5000res100gbs'
    # optimalitySimulation(5000, 1, name)
    allErrors = load_dataset(name)
    # plotOptimality(allErrors)


bandits = openBandits()
nBandits = 7
trueValues = [0.1*i for i in range(nBandits)]
actions = [i for i in range(1, len(bandits)+1)]

if __name__=="__main__":
    from modules import *
    figure_2_2()
    figure_2_3()
    figure_2_3(my_bandits=True)
    figure_2_4()
    figure_2_4(my_bandits=True)
    figure_2_5()
    figure_2_6()
    figure_2_6( my_bandits=True)
    from Plotter import *
    my_plots()
